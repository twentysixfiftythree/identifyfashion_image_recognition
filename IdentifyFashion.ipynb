{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nPtRjDTWm9hyObegyNlxo8evtNpXy4cu",
      "authorship_tag": "ABX9TyPGO79EWHYst2qHyzHo0l3U"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using a CNN to Identify Fashion\n"
      ],
      "metadata": {
        "id": "s9EAv_AHvbNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/drive/MyDrive/archive\\(2\\).zip"
      ],
      "metadata": {
        "id": "unqRnmKb0FqJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Exploration and Label Distribution Analysis"
      ],
      "metadata": {
        "id": "wi_yreQ_58VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import os\n",
        "import glob\n",
        "from collections import Counter\n",
        "import random\n",
        "myseed = 12345  # set a random seed for reproducibility\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "base_path = './myntradataset/images/'\n",
        "\n",
        "# let's take a look at one random image\n",
        "\n",
        "random_pic_path = random.choice(os.listdir(os.path.join(base_path, )))\n",
        "pic = imageio.imread(os.path.join(base_path, random_pic_path))\n",
        "cv2_imshow(pic)\n",
        "height, width, channels = pic.shape\n",
        "print(f'Original height, width, and channels of each image: {height} {width} {channels}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "2ebe-UwR2ggP",
        "outputId": "1b8edcd0-b87f-4db8-cd0b-4559b4c3298b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-754dc42acd2b>:15: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  pic = imageio.imread(os.path.join(base_path, random_pic_path))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x80>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original height, width, and channels of each image: 80 60 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Libaries and GPU"
      ],
      "metadata": {
        "id": "IwtBBv_XCOz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
        "from torchvision.datasets import DatasetFolder, VisionDataset\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "1EI3KZr77UpS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic setup for PyTorch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ],
      "metadata": {
        "id": "evNUnygQCVYC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((80, 60)),  # all images are 80 x 60 anyways\n",
        "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert image to RGB\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "])\n",
        "# they're the same, but will be adjusted eventually for data augmentation\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((80, 60)),  # all images are 80 x 60 anyways\n",
        "    transforms.Lambda(lambda x: x.convert('RGB')),  # Convert image to RGB\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "])"
      ],
      "metadata": {
        "id": "audLTRjBCaIx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Class for DataLoader"
      ],
      "metadata": {
        "id": "jVkDOJUsDo-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_path, images_path, tfm=None):\n",
        "        \"\"\"\n",
        "        csv_path: Path to the train.csv or test.csv file.\n",
        "        images_path: Base path where the images are stored.\n",
        "        tfm: Transforms to be applied on the images.\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
        "        self.images_path = images_path\n",
        "        self.transform = tfm\n",
        "\n",
        "        # Creating a dictionary to convert label names to integers\n",
        "        self.label_dict = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(self.data.iloc[idx, 0])  #first column is id\n",
        "        label_name = self.data.iloc[idx, 1]  # second column is the label\n",
        "        img_path = os.path.join(self.images_path, img_name + \".jpg\")\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Ensure all images are in RGB\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.label_dict[label_name]\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "EHcJFZH0Chn7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Net Setup"
      ],
      "metadata": {
        "id": "eFvqii_DDXR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the dataset\n",
        "train_dataset = FashionDataset(\n",
        "    csv_path='/content/drive/MyDrive/CNN Fashion data/train.csv',\n",
        "    images_path='/content/images',\n",
        "    tfm=train_transforms\n",
        ")"
      ],
      "metadata": {
        "id": "fdqxKJwnDrPC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdjustedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdjustedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1) # output: 80x60\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # output: 40x30\n",
        "        # after two pooling layers, the size will be 20x15 (80 -> 40 -> 20, 60 -> 30 -> 15)\n",
        "        self.fc1 = nn.Linear(64 * 20 * 15, 128)\n",
        "        self.fc2 = nn.Linear(128, 13) # 13 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # pooling halves the dimensions: 80x60 -> 40x30\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # again halving: 40x30 -> 20x15\n",
        "        x = x.view(-1, 64 * 20 * 15) # flatten the output for the fully connected layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0vgwjsBXBC3_"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = AdjustedCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9QMBdYvNBgxc"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVxLhifQjJYs",
        "outputId": "abc48cb9-2a8f-443f-9c0d-8290935799b8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5106\n",
            "Epoch [2/10], Loss: 0.7534\n",
            "Epoch [3/10], Loss: 0.2266\n",
            "Epoch [4/10], Loss: 0.2449\n",
            "Epoch [5/10], Loss: 0.2307\n",
            "Epoch [6/10], Loss: 0.0577\n",
            "Epoch [7/10], Loss: 0.0336\n",
            "Epoch [8/10], Loss: 0.0625\n",
            "Epoch [9/10], Loss: 0.0120\n",
            "Epoch [10/10], Loss: 0.0134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = FashionDataset(\n",
        "    csv_path='/content/drive/MyDrive/CNN Fashion data/test.csv',\n",
        "    images_path='/content/images',\n",
        "    tfm=test_transforms\n",
        "\n",
        ")\n",
        "label_dict = test_dataset.label_dict"
      ],
      "metadata": {
        "id": "kDJ2fcRYjmgR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "QbAm07ki2qqG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk2Oxlyp2u1W",
        "outputId": "91aea3a5-0e3e-48dd-b526-74e4e4b226cc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the test images: 99.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on random individual images"
      ],
      "metadata": {
        "id": "pUjzZaghAo36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# if you want to test on an 80x60 image, put it in here\n",
        "image_base_path = '/content/images'\n",
        "image_file_name = '33262.jpg'\n",
        "image_path = os.path.join(image_base_path, image_file_name)"
      ],
      "metadata": {
        "id": "ZG-NoHBE_DyE"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "\n",
        "transformed_image = test_transforms(image)\n",
        "\n",
        "# Add a batch dimension\n",
        "transformed_image = transformed_image.unsqueeze(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "transformed_image = transformed_image.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(transformed_image)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predicted_index = predicted.item()\n",
        "\n",
        "\n",
        "idx_to_label = {v: k for k, v in label_dict.items()}\n",
        "\n",
        "\n",
        "predicted_label = idx_to_label[predicted_index]\n",
        "\n",
        "print(f'The model predicts: {predicted_label}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bc8-r4g_deQ",
        "outputId": "be61a6a4-dec4-4000-ee69-d4b7fa7fd5a7"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model predicts: Topwear\n"
          ]
        }
      ]
    }
  ]
}